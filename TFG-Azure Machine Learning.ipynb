{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "With support of UPC and Microsoft Corporation.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully convolutional networks for image segmentation\n",
    "\n",
    "### This field is part of final batchelor tesis for an academic degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train configuration\n",
    "\n",
    "Define primary values to train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 224x224 configuration\n"
     ]
    }
   ],
   "source": [
    "# --- Define values to execute a SINGLE EXPERIMENT ---\n",
    "single_experiment = True\n",
    "architecture_name = 'FCN-8'\n",
    "#Accepted values: FCN-8, UNET\n",
    "model_name= 'VGG-16'\n",
    "#Accepted values: VGG-16, Resnet-50, MobileNet\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "n_classes = 3\n",
    "image_size = 224\n",
    "# Accepted values: 224, 256, 384, 512\n",
    "\n",
    "if(image_size == 256 or image_size == 224):\n",
    "    batch_size = 40\n",
    "    steps_per_epoch = 28\n",
    "    val_batch_size = 70\n",
    "    val_steps_per_epoch = 2\n",
    "    epochs = 100\n",
    "    print('Loaded 224x224 configuration')\n",
    "elif(image_size == 384):\n",
    "    batch_size = 40\n",
    "    steps_per_epoch = 28\n",
    "    val_batch_size = 70\n",
    "    val_steps_per_epoch = 2\n",
    "    epochs = 100\n",
    "    print('Loaded 384x384 configuration')\n",
    "elif(image_size == 512):\n",
    "    batch_size = 28\n",
    "    steps_per_epoch = 40\n",
    "    val_batch_size = 28\n",
    "    val_steps_per_epoch = 5\n",
    "    epochs = 100\n",
    "    print('Loaded 512x512 configuration')\n",
    "else:\n",
    "    batch_size = 10\n",
    "    val_batch_size = 10\n",
    "    steps_per_epoch = 5\n",
    "    val_steps_per_epoch = 5\n",
    "    epochs = 25\n",
    "    print('Loaded default configuration to quick test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection & execution params\n",
    "Define connection and executions params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1539,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder_name = \"TFG\"\n",
    "# Accepted values: 'VGG-16', 'Resnet-50', 'MobileNet'\n",
    "\n",
    "# Data input prefix associated a the data input name defined\n",
    "# Our data name is defined by \"size_step_type_\" for example \"384_train_img\" or \"384_train_ann\"\n",
    "storage_iput_prefix = str(image_size)\n",
    "\n",
    "# experiment_name - example: 'VGG-16-384x384'\n",
    "experiment_name = experiment_folder_name+'-' + \\\n",
    "    storage_iput_prefix+'x'+storage_iput_prefix\n",
    "\n",
    "# Execution instance name\n",
    "gpu_instance = \"k80-lp\"\n",
    "\n",
    "# Storage credentials\n",
    "subscription_id = '628b0476-bda5-4357-ae22-3c9caf0a760b'\n",
    "resource_group = 'azure-ml-resource'\n",
    "workspace_name = 'tfg-workspace'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create data & environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "\n",
    "Import Python packages you need in this session. Also display the Azure Machine Learning SDK version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.6.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "\n",
    "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `ws`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfg-workspace\twestus2\tazure-ml-resource\n"
     ]
    }
   ],
   "source": [
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create experiment\n",
    "\n",
    "Create an experiment to track the runs in your workspace. A workspace can have muliple experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1542,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. You will submit Python code to run on this VM later in the tutorial. \n",
    "The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target: k80-lp\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# set a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", gpu_instance)\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 1)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC6\")\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(\"found compute target: \" + compute_name)\n",
    "else:\n",
    "    print(\"creating new compute target...\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes,\n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(\n",
    "        ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TFG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged specified data into generic experiment dataset container\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "_overwrite = True\n",
    "load_data_in_local = False\n",
    "\n",
    "if load_data_in_local:\n",
    "    data_folder = os.path.join(os.getcwd(), 'dataset/384x384')\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    img_train_path = os.path.join(data_folder, 'img_train')\n",
    "    ann_train_path = os.path.join(data_folder, 'ann_train')\n",
    "    img_val_path = os.path.join(data_folder, 'img_val')\n",
    "    ann_val_path = os.path.join(data_folder, 'ann_val')\n",
    "    img_test_path = os.path.join(data_folder, 'img_test')\n",
    "    ann_test_path = os.path.join(data_folder, 'ann_test')\n",
    "\n",
    "    img_train.download(target_path=img_train_path, overwrite=_overwrite)\n",
    "    ann_train.download(target_path=ann_train_path, overwrite=_overwrite)\n",
    "    img_val.download(target_path=img_val_path, overwrite=_overwrite)\n",
    "    ann_val.download(target_path=ann_val_path, overwrite=_overwrite)\n",
    "    img_test.download(target_path=img_test_path, overwrite=_overwrite)\n",
    "    ann_test.download(target_path=ann_test_path, overwrite=_overwrite)\n",
    "    print('Loaded data in local')\n",
    "\n",
    "else:\n",
    "    img_train = Dataset.get_by_name(\n",
    "        workspace, name=storage_iput_prefix+'_train_img')\n",
    "    ann_train = Dataset.get_by_name(\n",
    "        workspace, name=storage_iput_prefix+'_train_ann')\n",
    "\n",
    "    img_val = Dataset.get_by_name(\n",
    "        workspace, name=storage_iput_prefix+'_val_img')\n",
    "    ann_val = Dataset.get_by_name(\n",
    "        workspace, name=storage_iput_prefix+'_val_ann')\n",
    "    img_test = Dataset.get_by_name(\n",
    "        workspace, name=storage_iput_prefix+'_test_img')\n",
    "    ann_test = Dataset.get_by_name(\n",
    "        workspace, name=storage_iput_prefix+'_test_ann')\n",
    "\n",
    "    img_train = img_train.register(workspace=ws,\n",
    "                                   name='img_train',\n",
    "                                   description='img_train',\n",
    "                                   create_new_version=True)\n",
    "    ann_train = ann_train.register(workspace=ws,\n",
    "                                   name='ann_train',\n",
    "                                   description='ann_train',\n",
    "                                   create_new_version=True)\n",
    "    img_val = img_val.register(workspace=ws,\n",
    "                               name='img_val',\n",
    "                               description='img_val',\n",
    "                               create_new_version=True)\n",
    "    ann_val = ann_val.register(workspace=ws,\n",
    "                               name='ann_val',\n",
    "                               description='ann_val',\n",
    "                               create_new_version=True)\n",
    "    img_test = img_test.register(workspace=ws,\n",
    "                                 name='img_test',\n",
    "                                 description='img_test',\n",
    "                                 create_new_version=True)\n",
    "    ann_test = ann_test.register(workspace=ws,\n",
    "                                 name='ann_test',\n",
    "                                 description='ann_test',\n",
    "                                 create_new_version=True)\n",
    "    print('Merged specified data into generic experiment dataset container')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1545,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), experiment_folder_name)\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script\n",
    "\n",
    "To submit the job to the cluster, first create a training script. Run the following code to create the training script called `train.py` in the directory you just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/users/histonzy/TFG/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import utils as my_utils\n",
    "from azureml.core import Run\n",
    "from keras_segmentation.models.unet import *\n",
    "from keras_segmentation.models.fcn import *\n",
    "from keras_segmentation.data_utils.data_loader import image_segmentation_generator\n",
    "from keras_segmentation.predict import evaluate\n",
    "\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "# get experiment inputs\n",
    "args = my_utils.azure_get_experiment_inputs()\n",
    "\n",
    "architecture_name = args.architecture_name\n",
    "model_name = args.model_name\n",
    "\n",
    "n_classes = args.n_classes\n",
    "img_train = args.img_train\n",
    "ann_train = args.ann_train\n",
    "img_val = args.img_val\n",
    "ann_val = args.ann_val\n",
    "img_test = args.img_test\n",
    "ann_test = args.ann_test\n",
    "batch_size = args.batch_size\n",
    "steps_per_epoch = args.steps_per_epoch\n",
    "val_batch_size = args.val_batch_size\n",
    "val_steps_per_epoch = args.val_steps_per_epoch\n",
    "epochs = args.epochs\n",
    "image_size = args.image_size\n",
    "optimizer_name = args.optimizer_name\n",
    "\n",
    "# define path to store checkpoints\n",
    "os.makedirs('outputs/checkpoint', exist_ok=True)\n",
    "checkpoint_path = 'outputs/checkpoint/'+model_name+'_1'\n",
    "\n",
    "# TRAIN\n",
    "if(architecture_name == 'FCN-8'):\n",
    "    if(model_name == 'VGG-16'):\n",
    "        model = fcn_8_vgg(n_classes=n_classes,\n",
    "                         input_height=image_size, input_width=image_size)\n",
    "    elif(model_name == 'Resnet-50'):\n",
    "        model = fcn_8_resnet50(n_classes=n_classes,\n",
    "                              input_height=image_size, input_width=image_size)\n",
    "    elif(model_name == 'MobileNet'):\n",
    "        model = fcn_8_mobilenet(n_classes=n_classes,\n",
    "                               input_height=image_size, input_width=image_size)\n",
    "    else:\n",
    "        raise Exception('Sorry, architecture name: {} no contains model name: {}').format(\n",
    "            architecture_name, model_name)\n",
    "\n",
    "elif(architecture_name == 'UNET'):\n",
    "    if(model_name == 'VGG-16'):\n",
    "        model = vgg_unet(n_classes=n_classes,\n",
    "                         input_height=image_size, input_width=image_size)\n",
    "    elif(model_name == 'Resnet-50'):\n",
    "        model = resnet50_unet(n_classes=n_classes,\n",
    "                              input_height=image_size, input_width=image_size)\n",
    "    elif(model_name == 'MobileNet'):\n",
    "        model = mobilenet_unet(n_classes=n_classes,\n",
    "                               input_height=image_size, input_width=image_size)\n",
    "    else:\n",
    "        raise Exception('Sorry, architecture name: {} no contains model name: {}').format(\n",
    "            architecture_name, model_name)\n",
    "else:\n",
    "    raise Exception('Sorry, architecture name: {} not found').format(\n",
    "        architecture_name)\n",
    "\n",
    "print('Selected train model is '+model_name +' & architecture is '+architecture_name)\n",
    "model.summary()\n",
    "\n",
    "model.train(\n",
    "    train_images=img_train,\n",
    "    train_annotations=ann_train,\n",
    "    val_images=img_val,\n",
    "    val_annotations=ann_val,\n",
    "    checkpoints_path=checkpoint_path,\n",
    "    validate=True,\n",
    "    batch_size=batch_size,\n",
    "    val_batch_size=val_batch_size,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    val_steps_per_epoch=val_steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    optimizer_name=optimizer_name\n",
    ")\n",
    "\n",
    "my_utils.save_checkpoints_in_zip()\n",
    "\n",
    "my_utils.azure_log_train_accuracy(model, run)\n",
    "my_utils.azure_log_plot_train_accuracy(model, run)\n",
    "\n",
    "\n",
    "# TEST\n",
    "evaluation = evaluate(model=model, inp_images_dir=img_test,\n",
    "                      annotations_dir=ann_test)\n",
    "\n",
    "my_utils.azure_log_test_data(evaluation, run)\n",
    "\n",
    "# Save model, the outputs folder is automatically uploaded into experiment record by AML Compute\n",
    "model.save('./outputs/model.h5')\n",
    "joblib.dump(value=model, filename='outputs/execution_model.pkl')\n",
    "# register model \n",
    "model_save = run.register_model(model_name='execution_'+model_name+'_'+architecture_name+'_'+str(image_size), model_path='outputs/execution_model.pkl')\n",
    "print(model_save.name, model_save.id, model_save.version, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a utils script\n",
    "\n",
    "Create a utils script `utils.py` to add support functionalities about train script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/users/histonzy/TFG/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/utils.py\n",
    "\n",
    "import glob\n",
    "import joblib\n",
    "import zipfile\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def azure_get_experiment_inputs():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    #dataset parser\n",
    "    parser.add_argument('--img_train', type=str,\n",
    "                        dest='img_train', help='img_train')\n",
    "    parser.add_argument('--ann_train', type=str,\n",
    "                        dest='ann_train', help='ann_train')\n",
    "    parser.add_argument('--img_val', type=str, dest='img_val', help='img_val')\n",
    "    parser.add_argument('--ann_val', type=str, dest='ann_val', help='ann_val')\n",
    "    parser.add_argument('--img_test', type=str,\n",
    "                        dest='img_test', help='img_test')\n",
    "    parser.add_argument('--ann_test', type=str,\n",
    "                        dest='ann_test', help='ann_test')\n",
    "    #train parser\n",
    "    parser.add_argument('--architecture_name', type=str,\n",
    "                        dest='architecture_name', help='architecture_name')\n",
    "    parser.add_argument('--model_name', type=str,\n",
    "                        dest='model_name', help='model_name')\n",
    "    parser.add_argument('--n_classes', type=int,\n",
    "                        dest='n_classes', default=3, help='n_classes')\n",
    "    parser.add_argument('--batch_size', type=int,\n",
    "                        dest='batch_size', default=10, help='batch size')\n",
    "    parser.add_argument('--steps_per_epoch', type=int,\n",
    "                        dest='steps_per_epoch', default=1, help='steps per epoch')\n",
    "    parser.add_argument('--val_batch_size', type=int,\n",
    "                        dest='val_batch_size', default=10, help='val batch size')\n",
    "    parser.add_argument('--val_steps_per_epoch', type=int,\n",
    "                        dest='val_steps_per_epoch', default=1, help='val steps per epoch')\n",
    "    parser.add_argument('--epochs', type=int, dest='epochs',\n",
    "                        default=5, help='epochs')\n",
    "    parser.add_argument('--image_size', type=int,\n",
    "                        dest='image_size', default=224, help='image size')\n",
    "    parser.add_argument('--optimizer_name', type=str,\n",
    "                        dest='optimizer_name', default=\"adadelta\", help='optimizer name')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def save_checkpoints_in_zip():\n",
    "\n",
    "    path = os.path.join(os.getcwd(), 'outputs')\n",
    "    path = os.path.abspath(os.path.normpath(os.path.expanduser(path)))\n",
    "    for folder in os.listdir(path):\n",
    "        zipf = zipfile.ZipFile('{0}.zip'.format(\n",
    "            os.path.join(path, folder)), 'w', zipfile.ZIP_DEFLATED)\n",
    "        for root, dirs, files in os.walk(os.path.join(path, folder)):\n",
    "            for filename in files:\n",
    "                zipf.write(os.path.abspath(os.path.join(\n",
    "                    root, filename)), arcname=filename)\n",
    "        zipf.close()\n",
    "\n",
    "def azure_log_train_accuracy(model, run):\n",
    "    run.log_list('Train accuracy -backup',\n",
    "                 model.history.history['accuracy'][:25], description='TRAIN ACCURACY')\n",
    "    run.log_list('Train accuracy -backup',\n",
    "                 model.history.history['accuracy'][25:50], description='TRAIN ACCURACY')\n",
    "    run.log_list('Train accuracy -backup',\n",
    "                 model.history.history['accuracy'][50:75], description='TRAIN ACCURACY')\n",
    "    run.log_list('Train accuracy -backup',\n",
    "                 model.history.history['accuracy'][75:], description='TRAIN ACCURACY')\n",
    "\n",
    "    run.log_list('Validation accuracy -backup',\n",
    "                 model.history.history['val_accuracy'][:25], description='VALIDATION ACCURACY')\n",
    "    run.log_list('Validation accuracy -backup',\n",
    "                 model.history.history['val_accuracy'][25:50], description='VALIDATION ACCURACY')\n",
    "    run.log_list('Validation accuracy -backup',\n",
    "                 model.history.history['val_accuracy'][50:75], description='VALIDATION ACCURACY')\n",
    "    run.log_list('Validation accuracy -backup',\n",
    "                 model.history.history['val_accuracy'][75:], description='VALIDATION ACCURACY')\n",
    "\n",
    "    run.log_list('Train loss -backup',\n",
    "                 model.history.history['loss'][:25], description='TRAIN LOSS')\n",
    "    run.log_list('Train loss -backup',\n",
    "                 model.history.history['loss'][25:50], description='TRAIN LOSS')\n",
    "    run.log_list('Train loss -backup',\n",
    "                 model.history.history['loss'][50:75], description='TRAIN LOSS')\n",
    "    run.log_list('Train loss -backup',\n",
    "                 model.history.history['loss'][75:], description='TRAIN LOSS')\n",
    "\n",
    "    run.log_list('Validation loss -backup',\n",
    "                 model.history.history['val_loss'][:25], description='VALIDATION LOSS')\n",
    "    run.log_list('Validation loss -backup',\n",
    "                 model.history.history['val_loss'][25:50], description='VALIDATION LOSS')\n",
    "    run.log_list('Validation loss -backup',\n",
    "                 model.history.history['val_loss'][50:75], description='VALIDATION LOSS')\n",
    "    run.log_list('Validation loss -backup',\n",
    "                 model.history.history['val_loss'][75:], description='VALIDATION LOSS')\n",
    "\n",
    "    run.log(\"final_val_loss\", model.history.history[\"val_loss\"][-1])\n",
    "\n",
    "\n",
    "def azure_log_plot_train_accuracy(model, run):\n",
    "    plt.figure(0)\n",
    "    plt.plot(model.history.history['accuracy'], 'b', label=\"accuracy\")\n",
    "    plt.plot(model.history.history['val_accuracy'], 'g', label=\"val_accuracy\")\n",
    "    plt.xticks(np.arange(0, 101, 5))\n",
    "    plt.yticks(np.arange(0, 1, 0.05))\n",
    "    plt.xlabel(\"Num of epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training accuracy vs Validation accuracy\")\n",
    "    plt.legend(['train', 'validation'])\n",
    "    plt.grid(True)\n",
    "    run.log_image(\"accuracy vs val_accuracy\", plot=plt)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(model.history.history['loss'], 'b')\n",
    "    plt.plot(model.history.history['val_loss'], 'g')\n",
    "    plt.xticks(np.arange(0, 101, 5))\n",
    "    #plt.yticks(np.arange(0, 10, 0.2))\n",
    "    plt.xlabel(\"Num of epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training loss vs Validation loss\")\n",
    "    plt.legend(['train', 'validation'])\n",
    "    plt.grid(True)\n",
    "    run.log_image(\"training loss vs validation loss\", plot=plt)\n",
    "\n",
    "\n",
    "def azure_log_test_data(evaluation, run):\n",
    "    run.log('frequency_weighted_IU', evaluation['frequency_weighted_IU'])\n",
    "    run.log('mean_IU', evaluation['mean_IU'])\n",
    "    run.log('Ground mean_IoU ', evaluation['class_wise_IU'][0])\n",
    "    run.log('Tree mean_IoU ', evaluation['class_wise_IU'][1])\n",
    "    run.log('Grass mean_IoU ', evaluation['class_wise_IU'][2])\n",
    "    run.log_list('class_wise_IU', evaluation['class_wise_IU'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/users/histonzy/TFG/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/evaluation.py\n",
    "\n",
    "import joblib\n",
    "from keras_segmentation.predict import evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define script params\n",
    "\n",
    "Next, we define script params to pass our defined train inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1549,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--n_classes': n_classes,\n",
    "    '--architecture_name': architecture_name,\n",
    "    '--model_name': model_name,\n",
    "    '--img_train': img_train.as_named_input('img_train').as_mount(),\n",
    "    '--ann_train': ann_train.as_named_input('ann_train').as_mount(),\n",
    "    '--img_val': img_val.as_named_input('img_val').as_mount(),\n",
    "    '--ann_val': ann_val.as_named_input('ann_val').as_mount(),\n",
    "    '--img_test': img_test.as_named_input('img_test').as_mount(),\n",
    "    '--ann_test': ann_test.as_named_input('ann_test').as_mount(),\n",
    "    '--batch_size': batch_size,\n",
    "    '--val_batch_size': val_batch_size,\n",
    "    '--steps_per_epoch': steps_per_epoch,\n",
    "    '--val_steps_per_epoch': val_steps_per_epoch,\n",
    "    '--epochs': epochs,\n",
    "    '--image_size': image_size\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tensorflow estimator & add dependencies\n",
    "\n",
    "Define a Tensorflow context to execute our experiment (`train.py`) over GPU using train input specifications (`script_params`) and defined dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1550,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - framework_version is not specified, defaulting to version 1.13.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 entry_script='train.py',\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target,\n",
    "                 pip_packages=[\n",
    "                     \"keras\",\n",
    "                     \"imageio\",\n",
    "                     \"imgaug\",\n",
    "                     \"opencv-python\",\n",
    "                     \"tqdm\",\n",
    "                     \"joblib\",\n",
    "                     \"matplotlib\"],\n",
    "                 use_gpu=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute individual experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1551,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(single_experiment):\n",
    "    run = exp.submit(est)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Imrpove experiment executions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Hyperparameters tunning\n",
    "\n",
    "Define tune params to test and experiment which combinations gets best performance.\n",
    "In this case, it will be used to make multiple experiment sequences using a different model for each iteration.\n",
    "\n",
    "Also, we can use a ´Grid Parameter Sampling´ to find best batch_size combination for each model and architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, GridParameterSampling\n",
    "from azureml.train.hyperdrive import choice\n",
    "\n",
    "find_best_batch_size = True\n",
    "\n",
    "if (find_best_batch_size):\n",
    "    ps = GridParameterSampling({\n",
    "        '--batch_size': choice(35,28,20,16,14,10,8,7,5,4),\n",
    "        '--architecture_name': choice('FCN-8', 'UNET'),\n",
    "        '--model_name': choice('Resnet-50', 'VGG-16'),\n",
    "        #   560,280,140,112,80,70,56,40,35,28,20,16,14,10,8,7,5,4\n",
    "    })\n",
    "else:\n",
    "    ps = RandomParameterSampling(\n",
    "        {\n",
    "            '--model_name': choice('VGG-16', 'Resnet-50'),\n",
    "            '--architecture_name': choice('FCN-8', 'UNET', 'MobileNet')\n",
    "            # '--optimizer_name': choice(\"SGD\", \"RMSprop\", \"Adam\", \"Adadelta\", \"Adagrad\", \"Adamax\", \"Nadam\")\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopper policies\n",
    "\n",
    "Define a stopper policies to perform our time execution and improve our financial expense\n",
    "\n",
    "`Median stopping` is an early termination policy based on running averages of primary metrics reported by the runs. This policy computes running averages across all training runs and terminates runs whose performance is worse than the median of the running averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import BanditPolicy, MedianStoppingPolicy\n",
    "\n",
    "#policy = BanditPolicy(evaluation_interval=50, slack_factor=0.1)\n",
    "policy = MedianStoppingPolicy(evaluation_interval=1, delay_evaluation=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Release and validate result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute experiment\n",
    "\n",
    "Finally, we'll define and hypedrive context and will execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import HyperDriveConfig, PrimaryMetricGoal\n",
    "\n",
    "hdc = HyperDriveConfig(estimator=est,\n",
    "                       hyperparameter_sampling=ps, \n",
    "                       policy=policy,\n",
    "                       primary_metric_name='Validation accuracy', \n",
    "                       primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                       max_total_runs=200,\n",
    "                       max_concurrent_runs=1)\n",
    "\n",
    "\n",
    "hdr = exp.submit(config=hdc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display run results\n",
    "\n",
    "You now have a model trained on a remote cluster.  Retrieve all the metrics logged during the run, including the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c6f73db889458e9a5c42979767008d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Canceled\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/TFG-512x512/runs/HD_9471c507-2bfb-4427-8b6e-47ecba327c9c?wsid=/subscriptions/628b0476-bda5-4357-ae22-3c9caf0a760b/resourcegroups/azure-ml-resource/workspaces/tfg-workspace\", \"run_id\": \"HD_9471c507-2bfb-4427-8b6e-47ecba327c9c\", \"run_properties\": {\"run_id\": \"HD_9471c507-2bfb-4427-8b6e-47ecba327c9c\", \"created_utc\": \"2020-06-29T00:02:54.947884Z\", \"properties\": {\"primary_metric_config\": \"{\\\"name\\\": \\\"Validation accuracy\\\", \\\"goal\\\": \\\"maximize\\\"}\", \"resume_from\": \"null\", \"runTemplate\": \"HyperDrive\", \"azureml.runsource\": \"hyperdrive\", \"platform\": \"AML\", \"ContentSnapshotId\": \"7bb41a8f-4295-48f5-8bca-0a48a3afeb0f\"}, \"tags\": {\"max_concurrent_jobs\": \"1\", \"max_total_jobs\": \"200\", \"max_duration_minutes\": \"10080\", \"policy_config\": \"{\\\"name\\\": \\\"MEDIANSTOPPING\\\", \\\"properties\\\": {\\\"evaluation_interval\\\": 1, \\\"delay_evaluation\\\": 20}}\", \"generator_config\": \"{\\\"name\\\": \\\"GRID\\\", \\\"parameter_space\\\": {\\\"--batch_size\\\": [\\\"choice\\\", [[35, 28, 20, 16, 14, 10, 8, 7, 5, 4]]], \\\"--architecture_name\\\": [\\\"choice\\\", [[\\\"FCN-8\\\", \\\"UNET\\\"]]], \\\"--model_name\\\": [\\\"choice\\\", [[\\\"Resnet-50\\\", \\\"VGG-16\\\"]]]}}\", \"primary_metric_config\": \"{\\\"name\\\": \\\"Validation accuracy\\\", \\\"goal\\\": \\\"maximize\\\"}\", \"platform_config\": \"{\\\"ServiceAddress\\\": \\\"https://westus2.experiments.azureml.net\\\", \\\"ServiceArmScope\\\": \\\"subscriptions/628b0476-bda5-4357-ae22-3c9caf0a760b/resourceGroups/azure-ml-resource/providers/Microsoft.MachineLearningServices/workspaces/tfg-workspace/experiments/TFG-512x512\\\", \\\"SubscriptionId\\\": \\\"628b0476-bda5-4357-ae22-3c9caf0a760b\\\", \\\"ResourceGroupName\\\": \\\"azure-ml-resource\\\", \\\"WorkspaceName\\\": \\\"tfg-workspace\\\", \\\"ExperimentName\\\": \\\"TFG-512x512\\\", \\\"Definition\\\": {\\\"Overrides\\\": {\\\"script\\\": \\\"train.py\\\", \\\"arguments\\\": [\\\"--n_classes\\\", 3, \\\"--img_train\\\", \\\"DatasetConsumptionConfig:img_train\\\", \\\"--ann_train\\\", \\\"DatasetConsumptionConfig:ann_train\\\", \\\"--img_val\\\", \\\"DatasetConsumptionConfig:img_val\\\", \\\"--ann_val\\\", \\\"DatasetConsumptionConfig:ann_val\\\", \\\"--img_test\\\", \\\"DatasetConsumptionConfig:img_test\\\", \\\"--ann_test\\\", \\\"DatasetConsumptionConfig:ann_test\\\", \\\"--val_batch_size\\\", 10, \\\"--steps_per_epoch\\\", 80, \\\"--val_steps_per_epoch\\\", 14, \\\"--epochs\\\", 100, \\\"--image_size\\\", 512], \\\"target\\\": \\\"k80-lp\\\", \\\"framework\\\": \\\"Python\\\", \\\"communicator\\\": \\\"None\\\", \\\"maxRunDurationSeconds\\\": null, \\\"nodeCount\\\": 1, \\\"environment\\\": {\\\"name\\\": null, \\\"version\\\": null, \\\"environmentVariables\\\": {\\\"EXAMPLE_ENV_VAR\\\": \\\"EXAMPLE_VALUE\\\"}, \\\"python\\\": {\\\"userManagedDependencies\\\": false, \\\"interpreterPath\\\": \\\"python\\\", \\\"condaDependenciesFile\\\": null, \\\"baseCondaEnvironment\\\": null, \\\"condaDependencies\\\": {\\\"name\\\": \\\"project_environment\\\", \\\"dependencies\\\": [\\\"python=3.6.2\\\", {\\\"pip\\\": [\\\"keras\\\", \\\"imageio\\\", \\\"imgaug\\\", \\\"opencv-python\\\", \\\"tqdm\\\", \\\"joblib\\\", \\\"matplotlib\\\", \\\"azureml-defaults\\\", \\\"azureml-dataprep[fuse,pandas]\\\", \\\"tensorflow-gpu==1.13.1\\\", \\\"horovod==0.16.1\\\"]}], \\\"channels\\\": [\\\"anaconda\\\", \\\"conda-forge\\\"]}}, \\\"docker\\\": {\\\"enabled\\\": true, \\\"baseImage\\\": \\\"mcr.microsoft.com/azureml/base-gpu:intelmpi2018.3-cuda10.0-cudnn7-ubuntu16.04\\\", \\\"baseDockerfile\\\": null, \\\"sharedVolumes\\\": true, \\\"shmSize\\\": \\\"2g\\\", \\\"arguments\\\": [], \\\"baseImageRegistry\\\": {\\\"address\\\": null, \\\"username\\\": null, \\\"password\\\": null, \\\"registryIdentity\\\": null}}, \\\"spark\\\": {\\\"repositories\\\": [], \\\"packages\\\": [], \\\"precachePackages\\\": false}, \\\"databricks\\\": {\\\"mavenLibraries\\\": [], \\\"pypiLibraries\\\": [], \\\"rcranLibraries\\\": [], \\\"jarLibraries\\\": [], \\\"eggLibraries\\\": []}, \\\"r\\\": null, \\\"inferencingStackVersion\\\": null}, \\\"history\\\": {\\\"outputCollection\\\": true, \\\"snapshotProject\\\": true, \\\"directoriesToWatch\\\": [\\\"logs\\\"]}, \\\"spark\\\": {\\\"configuration\\\": {\\\"spark.app.name\\\": \\\"Azure ML Experiment\\\", \\\"spark.yarn.maxAppAttempts\\\": 1}}, \\\"hdi\\\": {\\\"yarnDeployMode\\\": \\\"cluster\\\"}, \\\"tensorflow\\\": {\\\"workerCount\\\": 1, \\\"parameterServerCount\\\": 1}, \\\"mpi\\\": {\\\"processCountPerNode\\\": 1}, \\\"paralleltask\\\": {\\\"maxRetriesPerWorker\\\": 0, \\\"workerCountPerNode\\\": 1, \\\"terminalExitCodes\\\": null}, \\\"dataReferences\\\": {}, \\\"data\\\": {\\\"img_train\\\": {\\\"dataLocation\\\": {\\\"dataset\\\": {\\\"id\\\": \\\"c7c4db7d-e873-4451-ba17-eaba304d90df\\\"}, \\\"datapath\\\": null}, \\\"createOutputDirectories\\\": false, \\\"mechanism\\\": \\\"mount\\\", \\\"environmentVariableName\\\": \\\"img_train\\\", \\\"pathOnCompute\\\": null, \\\"overwrite\\\": false}, \\\"ann_train\\\": {\\\"dataLocation\\\": {\\\"dataset\\\": {\\\"id\\\": \\\"f822b489-2d90-4afd-bbbc-4195af1073b3\\\"}, \\\"datapath\\\": null}, \\\"createOutputDirectories\\\": false, \\\"mechanism\\\": \\\"mount\\\", \\\"environmentVariableName\\\": \\\"ann_train\\\", \\\"pathOnCompute\\\": null, \\\"overwrite\\\": false}, \\\"img_val\\\": {\\\"dataLocation\\\": {\\\"dataset\\\": {\\\"id\\\": \\\"a10419a1-9b63-41a0-a324-01261aac61bf\\\"}, \\\"datapath\\\": null}, \\\"createOutputDirectories\\\": false, \\\"mechanism\\\": \\\"mount\\\", \\\"environmentVariableName\\\": \\\"img_val\\\", \\\"pathOnCompute\\\": null, \\\"overwrite\\\": false}, \\\"ann_val\\\": {\\\"dataLocation\\\": {\\\"dataset\\\": {\\\"id\\\": \\\"33ede981-cb01-4243-b580-2c73fc54b700\\\"}, \\\"datapath\\\": null}, \\\"createOutputDirectories\\\": false, \\\"mechanism\\\": \\\"mount\\\", \\\"environmentVariableName\\\": \\\"ann_val\\\", \\\"pathOnCompute\\\": null, \\\"overwrite\\\": false}, \\\"img_test\\\": {\\\"dataLocation\\\": {\\\"dataset\\\": {\\\"id\\\": \\\"5172b2e6-4d6a-4997-9283-84bad094fa3e\\\"}, \\\"datapath\\\": null}, \\\"createOutputDirectories\\\": false, \\\"mechanism\\\": \\\"mount\\\", \\\"environmentVariableName\\\": \\\"img_test\\\", \\\"pathOnCompute\\\": null, \\\"overwrite\\\": false}, \\\"ann_test\\\": {\\\"dataLocation\\\": {\\\"dataset\\\": {\\\"id\\\": \\\"4cb859f1-2bb9-488d-8b75-f9aaed07df51\\\"}, \\\"datapath\\\": null}, \\\"createOutputDirectories\\\": false, \\\"mechanism\\\": \\\"mount\\\", \\\"environmentVariableName\\\": \\\"ann_test\\\", \\\"pathOnCompute\\\": null, \\\"overwrite\\\": false}}, \\\"sourceDirectoryDataStore\\\": null, \\\"amlcompute\\\": {\\\"vmSize\\\": null, \\\"vmPriority\\\": null, \\\"retainCluster\\\": false, \\\"name\\\": null, \\\"clusterMaxNodeCount\\\": 1}}, \\\"TargetDetails\\\": null, \\\"SnapshotId\\\": \\\"7bb41a8f-4295-48f5-8bca-0a48a3afeb0f\\\", \\\"TelemetryValues\\\": {\\\"amlClientType\\\": \\\"azureml-sdk-train\\\", \\\"amlClientModule\\\": \\\"[Scrubbed]\\\", \\\"amlClientFunction\\\": \\\"[Scrubbed]\\\", \\\"tenantId\\\": \\\"06334511-1919-441f-a170-c68a2287cb5a\\\", \\\"amlClientRequestId\\\": \\\"ad781d5d-e46f-49ee-a788-ca087f1e280d\\\", \\\"amlClientSessionId\\\": \\\"18069c2a-24c6-4491-aeb6-fd5e52f4f12e\\\", \\\"subscriptionId\\\": \\\"628b0476-bda5-4357-ae22-3c9caf0a760b\\\", \\\"estimator\\\": \\\"TensorFlow\\\", \\\"samplingMethod\\\": \\\"GRID\\\", \\\"terminationPolicy\\\": \\\"MedianStopping\\\", \\\"primaryMetricGoal\\\": \\\"maximize\\\", \\\"maxTotalRuns\\\": 200, \\\"maxConcurrentRuns\\\": 1, \\\"maxDurationMinutes\\\": 10080, \\\"vmSize\\\": null}}}\", \"resume_child_runs\": \"null\", \"all_jobs_generated\": \"true\", \"cancellation_requested\": \"true\", \"progress_metadata_evaluation_timestamp\": \"\\\"2020-06-29T00:02:55.969687\\\"\", \"progress_metadata_digest\": \"\\\"8819aeabcd322e67eaafae622f8e31f7147fc94804b9207418ecbad331175a63\\\"\", \"progress_metadata_active_timestamp\": \"\\\"2020-06-29T00:02:55.969687\\\"\", \"HD_9471c507-2bfb-4427-8b6e-47ecba327c9c_0\": \"{\\\"--batch_size\\\": 35, \\\"--architecture_name\\\": \\\"FCN-8\\\", \\\"--model_name\\\": \\\"Resnet-50\\\"}\", \"environment_preparation_status\": \"PREPARED\", \"prepare_run_id\": \"HD_9471c507-2bfb-4427-8b6e-47ecba327c9c_preparation\", \"HD_9471c507-2bfb-4427-8b6e-47ecba327c9c_0_cancelled\": \"true\", \"final_best_metric_update_retry_count\": \"1\"}, \"end_time_utc\": \"2020-06-29T00:06:26.840349Z\", \"status\": \"Canceled\", \"log_files\": {\"azureml-logs/hyperdrive.txt\": \"https://tfgworkspace7829773736.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_9471c507-2bfb-4427-8b6e-47ecba327c9c/azureml-logs/hyperdrive.txt?sv=2019-02-02&sr=b&sig=OE7nVsGBJSioUSA0ilUh11jUeVEiqZ9ij%2BA4sQzBc%2Fk%3D&st=2020-06-28T23%3A56%3A38Z&se=2020-06-29T08%3A06%3A38Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/hyperdrive.txt\"]], \"run_duration\": \"0:03:31\", \"hyper_parameters\": {\"--batch_size\": [\"choice\", [[35, 28, 20, 16, 14, 10, 8, 7, 5, 4]]], \"--architecture_name\": [\"choice\", [[\"FCN-8\", \"UNET\"]]], \"--model_name\": [\"choice\", [[\"Resnet-50\", \"VGG-16\"]]]}}, \"child_runs\": [{\"run_id\": \"HD_9471c507-2bfb-4427-8b6e-47ecba327c9c_0\", \"run_number\": 12, \"metric\": null, \"status\": \"Canceled\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-06-29T00:03:38.996917Z\", \"end_time\": \"2020-06-29T00:05:38.928633Z\", \"created_time\": \"2020-06-29T00:03:28.287244Z\", \"created_time_dt\": \"2020-06-29T00:03:28.287244Z\", \"duration\": \"0:02:10\", \"hyperdrive_id\": \"9471c507-2bfb-4427-8b6e-47ecba327c9c\", \"arguments\": null, \"param_--batch_size\": 35, \"param_--architecture_name\": \"FCN-8\", \"param_--model_name\": \"Resnet-50\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2020-06-29T00:02:55.403248][API][INFO]Experiment created\\r\\n[2020-06-29T00:02:56.226331][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\\r\\n[2020-06-29T00:02:56.6682833Z][SCHEDULER][INFO]The execution environment is being prepared. Please be patient as it can take a few minutes.\\r\\n[2020-06-29T00:02:56.593680][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\\r\\n[2020-06-29T00:03:27.6160323Z][SCHEDULER][INFO]The execution environment was successfully prepared.\\r\\n[2020-06-29T00:03:27.6001258Z][SCHEDULER][INFO]Scheduling job, id='HD_9471c507-2bfb-4427-8b6e-47ecba327c9c_0'\\r\\n[2020-06-29T00:03:28.4381513Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_9471c507-2bfb-4427-8b6e-47ecba327c9c_0'\\r\\n[2020-06-29T00:04:36.260623][ENFORCER][INFO]Jobs [https://westus2.experiments.azureml.net/subscriptions/628b0476-bda5-4357-ae22-3c9caf0a760b/resourceGroups/azure-ml-resource/providers/Microsoft.MachineLearningServices/workspaces/tfg-workspace/experiments/**SCRUBBED**/runs/HD_9471c507-2bfb-4427-8b6e-47ecba327c9c_0] do not contain any metrics with the primary metric name at this moment, policy cannot be applied.\\r\\n[2020-06-29T00:05:07.020592][ENFORCER][INFO]Jobs [https://westus2.experiments.azureml.net/subscriptions/628b0476-bda5-4357-ae22-3c9caf0a760b/resourceGroups/azure-ml-resource/providers/Microsoft.MachineLearningServices/workspaces/tfg-workspace/experiments/**SCRUBBED**/runs/HD_9471c507-2bfb-4427-8b6e-47ecba327c9c_0] do not contain any metrics with the primary metric name at this moment, policy cannot be applied.\\r\\n[2020-06-29T00:05:16.573049][API][INFO]Experiment has been marked as canceled by the user, all active/pending jobs will soon be terminated/canceled.\\r\\n[2020-06-29T00:05:16.733445][API][INFO]Processing cancellation request by the user.\\r\\n[2020-06-29T00:05:30.9747751Z][SCHEDULER][INFO]Cancelling job, id='HD_9471c507-2bfb-4427-8b6e-47ecba327c9c_0'\\r\\n[2020-06-29T00:05:31.9031079Z][SCHEDULER][INFO]Updating job statuses to cancelled: [(job id = 'HD_9471c507-2bfb-4427-8b6e-47ecba327c9c_0', previous status = 'RUNNING')]\\r\\n[2020-06-29T00:05:36.984427][ENFORCER][INFO]Jobs [https://westus2.experiments.azureml.net/subscriptions/628b0476-bda5-4357-ae22-3c9caf0a760b/resourceGroups/azure-ml-resource/providers/Microsoft.MachineLearningServices/workspaces/tfg-workspace/experiments/**SCRUBBED**/runs/HD_9471c507-2bfb-4427-8b6e-47ecba327c9c_0] do not contain any metrics with the primary metric name at this moment, policy cannot be applied.\\r\\n[2020-06-29T00:06:27.116888][CONTROLLER][INFO]Experiment was 'ExperimentStatus.CANCEL_REQUESTED', is 'ExperimentStatus.CANCELLED'.\\r\\n[2020-06-29T00:06:27.658132][CONTROLLER][INFO]Experiment was 'ExperimentStatus.CANCEL_REQUESTED', is 'ExperimentStatus.CANCELLED'.\\n\\nRun is canceled.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.6.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(hdr).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check best result and best params combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr.wait_for_completion(show_output=True)\n",
    "assert(hdr.get_status() == \"Completed\")\n",
    "best_run = hdr.get_best_run_by_primary_metric()\n",
    "print(best_run.get_details()['runDefinition']['arguments'])"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
